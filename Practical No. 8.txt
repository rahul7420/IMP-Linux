Practical No. 8 ‚Äì Implementation of Autoencoder
Aim:

To implement an Autoencoder using PyTorch for image reconstruction and feature extraction on the MNIST dataset.
****************************************************************************************************************************

1) Brief summary of what you ran

Built a SimpleNN MLP with num_layers hidden layers, each of size hidden_size, ReLU activations, and a final linear head for 10 classes.

Split MNIST into 80/20 train/val using random_split.

For each config you trained for 5 epochs with Adam and returned final training accuracy (from the last epoch) and validation accuracy.

Results (best ‚Üí config 3): more depth and larger hidden size improved both train and val accuracy:

Config 1: train 95.81%, val 94.66%

Config 2: train 96.01%, val 95.71%

Config 3: train 96.91%, val 96.88% ‚Üê best

Config 4: train 95.97%, val 95.79%

2) Quick interpretation of results

Trend: Increasing capacity (2 layers √ó 128 units) + larger batch gave the best performance. That‚Äôs expected: MNIST is easy and benefits from extra capacity up to a limit.

Batch size effect: config 2 (batch 64) slightly improved val vs config 1 (batch 32) with same other hyperparams ‚Äî sometimes larger batch size gives smoother gradient and slightly different generalization.

Learning rate: config 4 lowered LR to 0.0005 but same capacity as some others ‚Äî lower LR slowed learning and yielded slightly lower train accuracy though val stayed comparable.

No severe overfitting after 5 epochs ‚Äî training and validation accuracies are close, indicating a stable regime.

3) Small caveats / code notes I noticed

!pip install wandb -q ran but you never used wandb in the loop ‚Äî if you intended to log experiments, I‚Äôll show how below.

model.summary() earlier in your TensorFlow run showed zero params because model was unbuilt ‚Äî not an issue here, but FYI when working with Keras.

You compute train_acc inside the epoch loop and return the last epoch‚Äôs train_acc ‚Äî that‚Äôs fine but usually we log per-epoch metrics (better for comparison/plots).

For faster DataLoader throughput on a machine with CPU cores / GPU, consider num_workers>0 and pin_memory=True (when using GPU).

For reproducibility, set seeds for torch, numpy, and random.

****************************************************************************************************************************
Theory:

An Autoencoder is a type of unsupervised neural network that learns to compress (encode) the input data into a smaller representation and then reconstruct (decode) it back to the original form.

It has two main parts:

Encoder: Compresses input data into a lower-dimensional space (latent space).

Decoder: Reconstructs the original data from the encoded representation.

Working of Autoencoder:

Input ‚Üí Encoder ‚Üí Latent Vector ‚Üí Decoder ‚Üí Output

The network is trained to minimize the reconstruction error between the input and output.

Mathematically,

Loss
=
‚à£
‚à£
ùëã
‚àí
ùëã
^
‚à£
‚à£
2
Loss=‚à£‚à£X‚àí
X
^
‚à£‚à£
2

where

ùëã
X = Original input

ùëã
^
X
^
 = Reconstructed output

Purpose of Autoencoders:

Dimensionality reduction (similar to PCA)

Noise removal (Denoising Autoencoders)

Feature extraction

Image compression and reconstruction

Types of Autoencoders:

Basic Autoencoder ‚Äì Simple encoder-decoder structure.

Denoising Autoencoder ‚Äì Learns to remove noise from data.

Sparse Autoencoder ‚Äì Forces sparsity in hidden layer for better feature learning.

Variational Autoencoder (VAE) ‚Äì Used for generating new data (like GANs).

Algorithm:

1.Import Libraries:
-Import torch, torch.nn, torch.optim, torchvision, matplotlib.

2.Load Dataset:

-Use the MNIST dataset.

-Normalize pixel values between 0 and 1.

-Create DataLoader for training and testing.

3.Define Autoencoder Architecture:

-Encoder: Fully connected layers that reduce input dimension.

-Decoder: Fully connected layers that reconstruct the input.
Example:

class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
        self.decoder = nn.Sequential(
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 784),
            nn.Sigmoid()
        )
    def forward(self, x):
        x = x.view(-1, 784)
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


4.Define Loss and Optimizer:

-Loss Function: Mean Squared Error (MSE)

-Optimizer: Adam or SGD

5.Training Loop:

-Forward pass ‚Üí Reconstruct output.

-Compute reconstruction loss (MSE).

-Backpropagate and update weights.

-Repeat for multiple epochs.

6.Testing:

-Pass test images through the trained model.

-Compare original vs reconstructed images.

7.Visualization:

-Display original and reconstructed images side by side.


Result:

The autoencoder successfully learned to reconstruct MNIST digit images.

Reconstructed images are similar to the original input.

Loss decreases with each epoch.

Example Output:

Epoch [20/20], Loss: 0.015
Reconstructed images closely match original inputs.

Conclusion:

The Autoencoder model efficiently learned compressed representations of the MNIST dataset.

It can be used for image compression, denoising, or feature extraction.

Autoencoders are a fundamental concept in unsupervised learning and form the basis of advanced models like Variational Autoencoders (VAE) and GANs.

Viva / Oral Questions and Answers:

Q1: What is an Autoencoder?
A: It is a type of neural network that learns to encode data into a lower-dimensional space and reconstruct it back.

Q2: What are the main parts of an Autoencoder?
A: Encoder and Decoder.

Q3: What is the purpose of the encoder?
A: To compress or encode input data into a smaller feature representation.

Q4: What is the purpose of the decoder?
A: To reconstruct the original data from the encoded representation.

Q5: What type of learning is used in Autoencoders?
A: Unsupervised learning.

Q6: What loss function is used?
A: Mean Squared Error (MSE) loss between input and reconstructed output.

Q7: What activation functions are used?
A: ReLU for hidden layers, Sigmoid for the output layer.

Q8: What optimizer is used?
A: Adam optimizer.

Q9: What dataset is used in this experiment?
A: MNIST handwritten digits dataset.

Q10: Mention one application of Autoencoders.

A: Denoising, feature extraction, or image compression.
