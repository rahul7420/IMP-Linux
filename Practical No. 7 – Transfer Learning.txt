Practical No. 7 â€“ Transfer Learning using Pretrained CNN
Aim:

To use Transfer Learning with a pretrained CNN model (like VGG16 or ResNet18) for image classification and compare its accuracy with a model trained from scratch.


*****************************************************************************************************************

ðŸŸ¦ A) Code theory â€” what each part does

Dataset load (tfds)

tfds.load('cats_vs_dogs', split=['train[:80%]','train[80%:]'], as_supervised=True) downloads TFDS Cats vs Dogs and splits into 80/20 train/val.

You observed some corrupted images were skipped â€” TFDS warns and automatically filters them.

Preprocessing pipeline

format_example resizes images to IMG_SIZE Ã— IMG_SIZE and normalizes pixels to [0,1].

.map(...).batch(BATCH_SIZE).prefetch(1) builds an efficient tf.data pipeline (prefetch helps hide I/O latency).

Backbone: VGG16 (pretrained)

VGG16(..., include_top=False, weights='imagenet') loads convolutional feature extractor without final classifier, initialized with ImageNet weights.

base_model.trainable = False freezes the convolutional layers so initial training only updates your new Dense heads.

Top (classifier) head

Flatten â†’ Dense(256, ReLU) â†’ Dropout(0.5) â†’ Dense(1, sigmoid) for binary classification.

Compile & initial training

optimizer='adam', loss='binary_crossentropy' and metrics=['accuracy'].

Training only the head (backbone frozen) for a few epochs learns classifier weights on VGG features.

Fine-tuning

base_model.trainable = True then freeze all but the last 4 convolutional layers allows the model to adapt the high-level conv filters to the cats-vs-dogs specifics.

Recompile with a lower learning rate (1e-5) to avoid destroying pretrained weights, then train further.

Plotting history

Combined history of head training + fine-tuning shows overall train/val trends across all epochs.

ðŸŸ© B) Output theory â€” what the numbers mean & interpretation

Observed numbers (summary)

Initial training (head only): train acc â‰ˆ 93.4%, val acc â‰ˆ 93.2% by epoch 5.

After fine-tuning last 4 layers: train acc reached ~99.2%, val acc â‰ˆ 96.0% (peak ~96.13%).

Validation loss decreased during fine-tuning initially, then rose slightly near end (possible mild overfitting).

Interpretation

Freezing backbone and training classifier head quickly gets you strong performance because ImageNet features transfer well.

Fine-tuning the top convolution layers improves validation accuracy further (from â‰ˆ93% â†’ â‰ˆ96%), showing the model adapted pretrained features to domain-specific patterns (cats vs dogs).

Very high train accuracy after fine-tuning (near 99%) with smaller improvements in val accuracy suggests capacity is high â€” monitor for overfitting if you continue training.

Validation-loss uptick late in fine-tuning suggests reducing LR, early stopping, or regularization might help.

*********************************************************************************************************************

Theory:

Transfer Learning means using a pretrained model (already trained on a large dataset like ImageNet) for a new problem with a smaller dataset.

Instead of training a network from zero, we reuse the learned weights and features from another model.

Why use Transfer Learning?

It reduces training time.

Works well even with small datasets.

Gives higher accuracy.

Prevents overfitting.

How it works:

Take a pretrained CNN (like VGG16 or ResNet18).

Keep the initial layers (that detect edges, shapes).

Replace the last layer with a new one for your dataset (e.g., 10 classes).

Train (fine-tune) only the last few layers.

Common pretrained models:

VGG16: Simple and deep.

ResNet18: Uses skip connections to avoid vanishing gradients.

MobileNet: Lightweight, fast for mobile devices.

Algorithm:

1.Import libraries â€“ torch, torchvision, torch.nn, matplotlib.

2.Load dataset â€“ CIFAR-10 or custom image dataset.

3.Apply transformations (resize, normalize).

4.Load pretrained model:

	model = models.resnet18(pretrained=True)


5.Freeze early layers and replace the last fully connected layer.

6.Define loss function (CrossEntropyLoss) and optimizer (Adam or SGD).

7.Train the model for a few epochs.

8.Test the model and calculate accuracy.

9.Plot loss and accuracy curves.



Result:

The pretrained CNN converged faster and gave better accuracy than a CNN trained from scratch.

Example:

Epoch 10: Train Accuracy = 98%, Test Accuracy = 97%

Conclusion:

Transfer Learning helps to reuse pretrained models to solve new problems quickly and efficiently.
It saves time, improves accuracy, and reduces overfitting when data is limited.

Viva Questions:

Q1: What is transfer learning?
A: Using a pretrained model on a new dataset.

Q2: Why do we use pretrained models?
A: To save time and get better accuracy with small datasets.

Q3: What is fine-tuning?
A: Retraining only some layers of the pretrained model on the new dataset.

Q4: What is the loss function used?
A: Cross Entropy Loss.

Q5: What optimizer is used?
A: Adam or SGD.

Q6: Name some pretrained CNN models.
A: VGG16, ResNet18, MobileNet, Inception.

Q7: What are frozen layers?
A: Layers whose weights are not updated during training.

Q8: What dataset is used?
A: CIFAR-10 or any small custom image dataset.

Q9: What is the advantage of transfer learning?
A: Faster training and better accuracy with less data.

Q10: Which model performed best?

A: ResNet18 performed best due to skip connections.
