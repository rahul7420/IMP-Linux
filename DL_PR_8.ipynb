{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOKnWfQMk7nlwA67nH8+jim"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yDwoqJRts02V","executionInfo":{"status":"ok","timestamp":1760428360422,"user_tz":-330,"elapsed":20770,"user":{"displayName":"Varad Joshi","userId":"01262868626950884000"}}},"outputs":[],"source":["# ✅ Install and import necessary libraries\n","!pip install wandb -q\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","import wandb\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","import pandas as pd\n","\n","# 1. Prepare Dataset\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_data, val_data = random_split(dataset, [train_size, val_size])\n","\n","# 2. Define model with variable layers\n","class SimpleNN(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, output_size):\n","        super(SimpleNN, self).__init__()\n","        layers = []\n","        in_size = input_size\n","        for h in hidden_sizes:\n","            layers.append(nn.Linear(in_size, h))\n","            layers.append(nn.ReLU())\n","            in_size = h\n","        layers.append(nn.Linear(in_size, output_size))\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# 3. Training function\n","def train_model(learning_rate, batch_size, num_layers, hidden_size, epochs=5):\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_data, batch_size=batch_size)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = SimpleNN(28*28, [hidden_size]*num_layers, 10).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0\n","        correct = 0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            images = images.view(images.size(0), -1)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            preds = outputs.argmax(dim=1)\n","            correct += (preds == labels).sum().item()\n","\n","        train_acc = correct / len(train_loader.dataset)\n","\n","    # Validation accuracy\n","    model.eval()\n","    correct_val = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            images = images.view(images.size(0), -1)\n","            outputs = model(images)\n","            preds = outputs.argmax(dim=1)\n","            correct_val += (preds == labels).sum().item()\n","\n","    val_acc = correct_val / len(val_loader.dataset)\n","    return train_acc, val_acc\n","\n","# 4. Run experiments\n","configs = [\n","    {'learning_rate': 0.001, 'batch_size': 32, 'num_layers': 1, 'hidden_size': 64},\n","    {'learning_rate': 0.001, 'batch_size': 64, 'num_layers': 1, 'hidden_size': 64},\n","    {'learning_rate': 0.001, 'batch_size': 64, 'num_layers': 2, 'hidden_size': 128},\n","    {'learning_rate': 0.0005, 'batch_size': 32, 'num_layers': 2, 'hidden_size': 64},\n","]\n","\n","results = []\n","\n","for i, cfg in enumerate(configs):\n","    print(f\"Running config {i+1}/{len(configs)}: {cfg}\")\n","    train_acc, val_acc = train_model(**cfg, epochs=5)\n","    results.append({**cfg, 'train_accuracy': train_acc, 'val_accuracy': val_acc})\n","\n","# 5. Show results\n","df = pd.DataFrame(results)\n","print(\"\\nSummary of results:\")\n","print(df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ajQoi9jgtJRc","executionInfo":{"status":"ok","timestamp":1760428658081,"user_tz":-330,"elapsed":233592,"user":{"displayName":"Varad Joshi","userId":"01262868626950884000"}},"outputId":"8bbb754d-3fc8-4c01-f1df-a2bf519f85fe"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 18.5MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 500kB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 4.58MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 7.29MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Running config 1/4: {'learning_rate': 0.001, 'batch_size': 32, 'num_layers': 1, 'hidden_size': 64}\n","Running config 2/4: {'learning_rate': 0.001, 'batch_size': 64, 'num_layers': 1, 'hidden_size': 64}\n","Running config 3/4: {'learning_rate': 0.001, 'batch_size': 64, 'num_layers': 2, 'hidden_size': 128}\n","Running config 4/4: {'learning_rate': 0.0005, 'batch_size': 32, 'num_layers': 2, 'hidden_size': 64}\n","\n","Summary of results:\n","   learning_rate  batch_size  num_layers  hidden_size  train_accuracy  \\\n","0         0.0010          32           1           64        0.958104   \n","1         0.0010          64           1           64        0.960146   \n","2         0.0010          64           2          128        0.969125   \n","3         0.0005          32           2           64        0.959688   \n","\n","   val_accuracy  \n","0      0.946583  \n","1      0.957083  \n","2      0.968750  \n","3      0.957917  \n"]}]}]}