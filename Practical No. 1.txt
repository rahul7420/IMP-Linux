Practical No. 1 – Implementation of Perceptron for AND Gate
Aim:

To implement a Single Layer Perceptron using Python and NumPy for the AND logic gate and visualize its decision boundary.

Theory:

The Perceptron is the simplest type of artificial neural network used for binary classification.
It was introduced by Frank Rosenblatt in 1958.
The perceptron simulates a biological neuron that receives inputs, multiplies them with weights, adds a bias, and passes the result through an activation function to produce an output.

**************************************************************************************************

✅ 2. ORAL EXPLANATION (Simple – For External Viva)

“This code trains a perceptron to learn the AND logic gate.
First, I define the AND gate inputs and outputs.
Then I initialize the perceptron’s weights and bias randomly.
I use a step activation function that outputs either 0 or 1.
During training, for each input, the model calculates the predicted output and compares it with the actual output.
Based on the error, the perceptron updates its weights using the perceptron learning rule.
After repeating this for 20 epochs, the perceptron successfully learns the correct AND gate behavior.
Finally, I plot the decision boundary to show how the perceptron separates the classes.”

✅ 3. ORAL EXPLANATION (Technical – For Internal Viva)

“This code demonstrates a perceptron learning the AND gate using supervised learning.
I first create the 2-input dataset for the AND function.
The perceptron is initialized with random weights and bias.
I use the standard step activation function since AND is a linearly separable binary classification problem.
For each epoch, I compute the weighted sum (w.x + b), apply the step function, compute the error, and update the weights using the perceptron rule:
w = w + lr × error × x, and b = b + lr × error.
Over epochs, the error decreases and the perceptron converges to correct weights.
Finally, I plot the decision boundary derived from the learned weights, which correctly separates the class 1 point (1,1) from the others.”

*******************************************
✅ Summary of the Code
1. Dataset Creation

You created the AND gate dataset with 4 inputs:

(0,0),(0,1),(1,0),(1,1)

Labels are:
0,0,0,1

2. Perceptron Initialization

The perceptron starts with random weights and a random bias.

Learning rate = 0.1

Training epochs = 20

3. Training Process

For each input:

Compute:
z=w1x1+w2x2+bias

Apply step function to predict 0 or 1.

Compare with actual label → calculate error.

Update weights and bias using the perceptron learning rule:
w=w+lr×error×x
b=b+lr×error

Over multiple epochs, the perceptron adjusts itself to correctly learn the AND logic.

4. Output During Training

Each epoch prints updated weights and bias.

You see values gradually changing until they converge.

This shows the perceptron is learning correctly.

✅ Summary of Output (Final Decision Boundary Plot)

The final plot shows:

Input points of AND gate colored by their class (0 or 1).

A straight line that the perceptron learned → this line separates:

All 0 outputs (left/bottom side)

The single 1 output (top-right corner)

This means the perceptron successfully learned the AND gate, which is linearly separable.

******************************

Perceptron Model:

Each perceptron has:

Inputs: x₁, x₂, …, xₙ

Weights: w₁, w₂, …, wₙ

Bias: b

Activation function: Step function

Mathematically:
z = (w₁x₁ + w₂x₂ + … + wₙxₙ) + b

Output (y):
If z ≥ 0 → y = 1
Else → y = 0

Learning Rule:

The perceptron learns by updating its weights based on the error between the true and predicted output.

-The AND gate is linearly separable, meaning a straight line can separate the classes (0 and 1).

-Therefore, a single-layer perceptron can learn this logic perfectly.

Algorithm: (Perceptron Learning Algorithm)

1.Initialize weights and bias with small random values.

2.Set learning rate (η) and number of epochs.

3.For each epoch:
	a. For each training sample (xi, yi):
	i. Compute the weighted sum → z = w·xi + b
	ii. Apply step activation function:
	- If z ≥ 0 → y_pred = 1
	- Else → y_pred = 0
	iii. Compute error = yi − y_pred
	iv. Update weights → w = w + η × error × xi
	v. Update bias → b = b + η × error

4.Repeat steps until there are no errors or maximum epochs reached.

5.Plot the decision boundary and display final weights and bias.

Result:

After training, the perceptron correctly classifies the AND gate outputs.

Accuracy = 100%

The decision boundary separates the single output ‘1’ point from the ‘0’ points.

Example Output:
Epoch 10: Weights = [0.2, 0.1], Bias = -0.1

Conclusion:

The Perceptron successfully learns the AND gate because the data is linearly separable.
However, it cannot learn non-linearly separable problems like the XOR gate.
To handle such cases, multi-layer perceptrons (MLPs) are used.

Viva Questions and Answers:

Q: What is a perceptron?
A: A perceptron is a single-layer neural network used for binary classification.

Q: What is the learning rule of a perceptron?
A: w = w + η × (y_true − y_pred) × x

Q: Which activation function is used in a perceptron?
A: Step (threshold) activation function.

Q: Can a perceptron learn XOR logic?
A: No, because XOR is not linearly separable.

Q: What does the bias term do?
A: The bias shifts the decision boundary left or right.

Q: What happens if the learning rate is too high?
A: The model oscillates and may not converge properly.

Q: What is convergence in a perceptron?

A: When all samples are correctly classified and no weight updates are required.

