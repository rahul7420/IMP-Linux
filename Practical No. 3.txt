Practical No. 3 ‚Äì Implementation of Regularization Techniques (L2 and Dropout) in Neural Networks
Aim:

To implement and compare the performance of a neural network on the MNIST dataset using no regularization, L2 regularization, and Dropout regularization.

*******************************************************************************************

üü¶ A) THEORY OF THE CODE (WHAT THE CODE DOES)
1. Data Preparation

MNIST images (28√ó28 grayscale) are loaded.

Transformations:

Convert images to tensors.

Normalize them to range [-1, 1].

DataLoader creates shuffled mini-batches for training and sequential batches for testing.

This ensures efficient loading and stable training.

2. Model Definition ‚Äî MLP With Optional Regularization

The model is a Multi-Layer Perceptron (MLP) with:

Input layer = 784 neurons

Hidden Layer 1 ‚Üí 256 neurons + ReLU

Hidden Layer 2 ‚Üí 128 neurons + ReLU

Output layer ‚Üí 10 neurons (digits 0‚Äì9)

Dropout (0.5) is applied only when enabled.

Thus, the model can behave in 3 modes:

No regularization

L2 regularization

Dropout

3. Training + Evaluation Function

Each epoch has two phases:

(a) Training Phase

Model is set to train() ‚Üí dropout is active.

For every batch:

Forward pass

Loss = CrossEntropyLoss

Backpropagation

SGD optimizer updates weights

Also computes:

Training loss

Training accuracy

(b) Validation Phase (Test Loop)

Model is set to eval() ‚Üí dropout switches off.

No backprop.

Only computes:

Validation loss

Validation accuracy

Each epoch prints both.

4. Running 3 Experiments

You run training three times with:

(A) No Regularization

Normal model

No dropout

No L2

Optimizer: SGD(lr=0.01)

(B) L2 Regularization (Weight Decay)

Adds penalty for large weights

Forces the model to keep weights small

Makes the model more stable

Helps reduce overfitting

(C) Dropout

Randomly drops 50% neurons during training

Forces different paths through the network

Prevents co-adaptation of neurons

Greatly reduces overfitting

5. Visualization

The final curves compare:

Training loss

Validation loss

Training accuracy

Validation accuracy

for all three models.

This helps understand how regularization changes model behavior.

üü© B) THEORY OF THE OUTPUT (WHAT THE RESULTS MEAN)

The printed results show, for each epoch:

Train Loss | Train Accuracy | Validation Loss | Validation Accuracy


Each experiment (No-Reg, L2, Dropout) produces different behavior.

üìå 1. No Regularization ‚Äî Expected Output Behavior

Training loss decreases sharply.

Train accuracy becomes very high.

Validation accuracy improves at first but may stagnate.

Validation loss may start increasing ‚Üí overfitting.

Reason:
The model memorizes the training data without any penalty.

üìå 2. L2 Regularization ‚Äî Expected Output Behavior

Training loss decreases more slowly.

Train accuracy is slightly lower than No-Reg.

Validation loss decreases more smoothly.

Validation accuracy becomes more stable.

Reason:
L2 prevents weights from becoming too large ‚Üí better generalization.

üìå 3. Dropout ‚Äî Expected Output Behavior

Training loss is higher (because dropout makes training harder).

Training accuracy is lower.

Validation accuracy improves steadily and becomes very stable.

Overfitting is heavily reduced.

Reason:
Dropout forces the model to learn many independent pathways ‚Üí robust learning.

üéØ 4. Plot Interpretation
Loss Plot

No-Reg ‚Üí lowest train loss but highest validation loss

L2 ‚Üí moderate train & validation loss

Dropout ‚Üí higher train loss but lowest validation loss

Accuracy Plot

No-Reg ‚Üí highest train accuracy but validation accuracy fluctuates

L2 ‚Üí balanced

Dropout ‚Üí stable and often best validation accuracy

This demonstrates that:

Regularization improves generalization by reducing overfitting.

***********************************************************************************

Theory:
1Ô∏è‚É£ What is Regularization?

Regularization is a technique used to prevent overfitting in neural networks.
Overfitting happens when a model performs very well on training data but poorly on unseen (test) data because it memorizes instead of learning general patterns.

Regularization methods add constraints or noise during training so that the model learns simpler and more general features.

2Ô∏è‚É£ Types of Regularization Used:
A) L2 Regularization (Weight Decay):

Adds a penalty term to the loss function based on the sum of squares of weights.


It discourages large weight values, leading to smoother models and better generalization.

In PyTorch, it is applied using:

optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)

B) Dropout Regularization:

Randomly drops (turns off) some neurons during each training iteration.

This prevents the network from becoming too dependent on specific neurons.

During testing, all neurons are used but with reduced weights.

Dropout can be represented as:

where randomly selected activations are set to 0 during training
y=f(Wx+b)where randomly selected activations are set to 0 during training

In PyTorch, implemented as:

nn.Dropout(0.5)

3Ô∏è‚É£ Model Architecture:

Input layer: 784 neurons (28√ó28 pixels from MNIST images)

Hidden layers:

1st hidden layer ‚Üí 256 neurons

2nd hidden layer ‚Üí 128 neurons

Activation ‚Üí ReLU

Output layer: 10 neurons (for 10 digit classes)

Loss function: Cross Entropy Loss

Optimizer: SGD (Stochastic Gradient Descent)

4Ô∏è‚É£ Purpose of Experiment:

To compare how no regularization, L2 regularization, and dropout affect:

Training loss and accuracy

Validation (test) loss and accuracy

Model‚Äôs ability to generalize

**Algorithm:

1.Import libraries ‚Äì torch, torch.nn, torch.optim, matplotlib.

2.Prepare dataset ‚Äì MNIST dataset with normalization.

3.Create data loaders ‚Äì for training and testing sets.

4.Define MLP model:

	-2 hidden layers (256, 128 neurons)

	-Activation: ReLU

	-Dropout (0.5) applied optionally

5.Define loss function ‚Äì CrossEntropyLoss.

6.Define optimizer:

	-Without regularization: optim.SGD(model.parameters(), lr=0.01)

	-With L2: optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)

7.Train the model:

	-Forward pass ‚Üí compute predictions

	-Compute loss ‚Üí backpropagate

	-Update weights using optimizer

	-Record training loss and accuracy

8.Evaluate model:

-Compute validation loss and accuracy on test set.

9.Repeat experiments:

(A) Without Regularization

(B) With L2 Regularization

(C) With Dropout

10.Plot results:

-Training vs Validation Loss Curve

-Training vs Validation Accuracy Curve

11.Compare performances of all three approaches.

Result:

The results show that regularization techniques help the model generalize better and prevent overfitting.

Method	Training Accuracy	Validation Accuracy	Overfitting
No Regularization	High (may overfit)	Lower	Yes
L2 Regularization	Moderate	Better	Reduced
Dropout	Slightly lower train acc	Good validation acc	Prevented

Example Output:

Epoch 5/5 Train Loss: 0.165, Train Acc: 96.45% Val Loss: 0.150, Val Acc: 97.20%


The validation accuracy improves with L2 and Dropout, and the loss curves show more stability.

Conclusion:

L2 regularization penalizes large weights, encouraging simpler models.

Dropout prevents the network from relying too heavily on specific neurons, improving generalization.

Both methods reduce overfitting and help the model perform better on unseen data.

Regularization is essential for building robust and reliable neural networks.

Viva / Oral Questions and Answers:

Q1. What is regularization?
A technique used to reduce overfitting and improve model generalization.

Q2. What is L2 regularization?
It adds a penalty proportional to the square of the weights, preventing large weight values.

Q3. What is dropout?
A method where random neurons are deactivated during training to reduce overfitting.

Q4. Why does dropout work?
It prevents the network from becoming too dependent on specific neurons and forces it to learn redundant representations.

Q5. What is weight decay in PyTorch?
It is the parameter for L2 regularization inside the optimizer.

Q6. How does regularization affect training accuracy?
It slightly decreases training accuracy but improves test accuracy.

Q7. Which optimizer did you use here?
Stochastic Gradient Descent (SGD) with or without weight decay.

Q8. What dataset was used?
MNIST dataset ‚Äì handwritten digits from 0 to 9.

Q9. What activation function was used?
ReLU (Rectified Linear Unit) for hidden layers.

Q10. What loss function was used?

Cross Entropy Loss for multi-class classification.
