Practical No. 2 â€“ Implementation of Multilayer Perceptron (MLP)
Aim:

To implement a Multilayer Perceptron (MLP) using PyTorch for image classification on the MNIST or Fashion-MNIST dataset, and to plot the loss and accuracy curves.

***************************************************
ðŸŸ¦ A) THEORY OF THE CODE (What the code is doing)
1. Hyperparameters Setup

You define batch size, learning rate, number of epochs, hidden layer sizes, and activation function.

These control how the neural network learns and how fast it updates weights.

2. Dataset Loading & Preprocessing

You load MNIST or FashionMNIST datasets.

Images are converted to tensors and normalized to the range [-1, 1].

DataLoader groups images into batches for efficient training.

3. MLP Model Definition

You create a Multi-Layer Perceptron with:

Input layer of size 784 (28Ã—28)

Two hidden layers (256 â†’ 128 neurons)

Output layer (10 classes)

Each hidden layer uses the chosen activation function (ReLU by default).

Images are flattened before passing through the network.

4. Loss Function & Optimizer

CrossEntropyLoss: Measures error between predicted class scores and true labels.

Adam optimizer: Updates network weights using gradients.

5. Training Loop

For each epoch:

For each batch:

Forward pass â†’ model predicts class probabilities.

Loss is calculated.

Backpropagation computes gradients.

Adam optimizer updates the weights.

Training loss (average per epoch) is stored.

6. Testing (Evaluation Loop)

Model stops learning (no gradient calculation).

Predicts on the test dataset.

Calculates:

Test Loss

Accuracy = (correct predictions / total samples) Ã— 100

These values are stored for plotting.

7. Plotting Results

The code plots:

Training vs Test Loss

Test Accuracy vs Epochs

This provides a visual understanding of how well the model learned.


ðŸŸ© B) THEORY OF THE OUTPUT (What the results represent)
1. Terminal Output per Epoch

Example:

Epoch [1/10], Train Loss: 0.40, Test Loss: 0.35, Accuracy: 88.50%


This means:

Train Loss: how well the model learned from training data.

Test Loss: how well it performs on unseen data.

Accuracy: percentage of correctly classified test images.

As training continues:

Train loss should decrease.

Test loss should also decrease or stabilize.

Accuracy should increase.

2. Loss Curves

The Loss Curve shows:

Training loss gradually decreasing â†’ model is learning.

Test loss showing whether the model generalizes well.

If both go down â†’ good learning.

If test loss increases while train loss decreases â†’ overfitting.

3. Accuracy Curve

Shows how the modelâ€™s test accuracy improves each epoch.

For MNIST: Expected 95â€“98% accuracy.

For FashionMNIST: Expected 85â€“90%.

A smooth upward curve means the model is improving steadily.

***********************************************************

Theory:

A Multilayer Perceptron (MLP) is a type of feedforward artificial neural network that consists of three main layers:

Input layer

One or more hidden layers

Output layer

Each neuron in one layer is connected to every neuron in the next layer.
It is capable of learning non-linear relationships between input and output using activation functions.

Working of MLP:

The input layer receives raw data (for MNIST â†’ 28Ã—28 pixels = 784 inputs).

Data is passed through hidden layers where each neuron computes a weighted sum:

The result is passed through an activation function (like ReLU, Sigmoid, or Tanh).

The final layer uses the Softmax function to produce probabilities for each class.

The model is trained by minimizing the Cross Entropy Loss using an optimizer such as Adam or SGD.

Common Activation Functions:
Function	Formula	Output Range	Remarks
Sigmoid	1 / (1 + e^-x)	(0, 1)	Used for binary classification
Tanh	(e^x - e^-x) / (e^x + e^-x)	(-1, 1)	Centered version of Sigmoid
ReLU	max(0, x)	[0, âˆž)	Fast and avoids vanishing gradients
Loss Function:

For multi-class classification, Cross Entropy Loss is used:

Optimizer:

Optimizers adjust model weights based on gradients to minimize loss.

Common optimizers: Stochastic Gradient Descent (SGD) and Adam.

Dataset Used:

MNIST â€“ Handwritten digits (0â€“9), grayscale, size 28Ã—28.

Fashion-MNIST â€“ Images of clothes, shoes, bags, etc., grayscale, size 28Ã—28.

Algorithm: (MLP Training Steps)

1.Import libraries: torch, torchvision, matplotlib.

2.Set hyperparameters: learning rate, epochs, batch size, and activation function.

3.Load dataset: MNIST or Fashion-MNIST with normalization.

4.Create data loaders for training and testing sets.

5.Define the MLP model:

	-Flatten the input (28Ã—28 â†’ 784).

	-Add multiple fully connected layers.

	-Apply activation function after each layer.

	-Add output layer with 10 neurons (for 10 classes).

6.Define loss function: CrossEntropyLoss.

7.Define optimizer: Adam.

8.Training loop:

	-Forward pass: compute model output.

	-Compute loss between predicted and actual labels.

	-Backward pass: compute gradients.

	-Update weights using optimizer.

9.Testing phase:

	-Evaluate model on test data.

	-Calculate test loss and accuracy.

10.Plot results:

	-Training loss vs. epochs.

	-Test loss vs. epochs.

	-Accuracy vs. epochs.

Result:

The MLP model was successfully trained and tested on the MNIST/Fashion-MNIST dataset.

The model achieved around:

Accuracy: 95â€“98% (MNIST)

Accuracy: 85â€“90% (Fashion-MNIST)

Loss decreased steadily with epochs, and accuracy improved over time.

The plotted graphs clearly show the training and validation performance.

Conclusion:

The Multilayer Perceptron (MLP) is capable of learning complex, non-linear patterns in image data.
It successfully classifies handwritten digits and fashion items with high accuracy.
Using ReLU activation and Adam optimizer gives better convergence compared to Sigmoid or Tanh.

Viva / Oral Questions and Answers:

Q1. What is an MLP?
A Multilayer Perceptron is a feedforward neural network with one or more hidden layers between the input and output.

Q2. Why are activation functions used?
To introduce non-linearity, helping the network learn complex patterns.

Q3. Which activation function is most commonly used?
ReLU, because it avoids the vanishing gradient problem and is computationally efficient.

Q4. What loss function is used in MLP?
Cross Entropy Loss for multi-class classification.

Q5. What optimizer did you use?
Adam Optimizer (Adaptive Moment Estimation), as it converges faster than SGD.

Q6. Why do we normalize data?
To bring all pixel values to a common scale (âˆ’1 to 1) for stable and faster training.

Q7. What is the difference between MNIST and Fashion-MNIST?
MNIST has handwritten digits (0â€“9), while Fashion-MNIST contains grayscale clothing images.

Q8. What is the role of the learning rate?
It controls how much the weights are updated during training.

Q9. What happens if we increase the number of hidden layers?
The model may learn better representations but can also overfit or train slower.

Q10. Can an MLP handle image spatial structure well?

Not very efficiently â€” CNNs (Convolutional Neural Networks) handle spatial patterns better.
