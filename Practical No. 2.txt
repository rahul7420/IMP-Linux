Practical No. 2 â€“ Implementation of Multilayer Perceptron (MLP)
Aim:

To implement a Multilayer Perceptron (MLP) using PyTorch for image classification on the MNIST or Fashion-MNIST dataset, and to plot the loss and accuracy curves.

Theory:

A Multilayer Perceptron (MLP) is a type of feedforward artificial neural network that consists of three main layers:

Input layer

One or more hidden layers

Output layer

Each neuron in one layer is connected to every neuron in the next layer.
It is capable of learning non-linear relationships between input and output using activation functions.

Working of MLP:

The input layer receives raw data (for MNIST â†’ 28Ã—28 pixels = 784 inputs).

Data is passed through hidden layers where each neuron computes a weighted sum:

ğ‘§
=
(
ğ‘¤
1
ğ‘¥
1
+
ğ‘¤
2
ğ‘¥
2
+
.
.
.
+
ğ‘¤
ğ‘›
ğ‘¥
ğ‘›
)
+
ğ‘
z=(w
1
	â€‹

x
1
	â€‹

+w
2
	â€‹

x
2
	â€‹

+...+w
n
	â€‹

x
n
	â€‹

)+b

The result is passed through an activation function (like ReLU, Sigmoid, or Tanh).

The final layer uses the Softmax function to produce probabilities for each class.

The model is trained by minimizing the Cross Entropy Loss using an optimizer such as Adam or SGD.

Common Activation Functions:
Function	Formula	Output Range	Remarks
Sigmoid	1 / (1 + e^-x)	(0, 1)	Used for binary classification
Tanh	(e^x - e^-x) / (e^x + e^-x)	(-1, 1)	Centered version of Sigmoid
ReLU	max(0, x)	[0, âˆ)	Fast and avoids vanishing gradients
Loss Function:

For multi-class classification, Cross Entropy Loss is used:

ğ¿
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘¦
ğ‘–
log
â¡
(
ğ‘¦
ğ‘–
^
)
L=âˆ’
i=1
âˆ‘
n
	â€‹

y
i
	â€‹

log(
y
i
	â€‹

^
	â€‹

)

where

ğ‘¦
ğ‘–
y
i
	â€‹

 = true label

ğ‘¦
ğ‘–
^
y
i
	â€‹

^
	â€‹

 = predicted probability for class i

Optimizer:

Optimizers adjust model weights based on gradients to minimize loss.

Common optimizers: Stochastic Gradient Descent (SGD) and Adam.

Dataset Used:

MNIST â€“ Handwritten digits (0â€“9), grayscale, size 28Ã—28.

Fashion-MNIST â€“ Images of clothes, shoes, bags, etc., grayscale, size 28Ã—28.

Algorithm: (MLP Training Steps)

1.Import libraries: torch, torchvision, matplotlib.

2.Set hyperparameters: learning rate, epochs, batch size, and activation function.

3.Load dataset: MNIST or Fashion-MNIST with normalization.

4.Create data loaders for training and testing sets.

5.Define the MLP model:

	-Flatten the input (28Ã—28 â†’ 784).

	-Add multiple fully connected layers.

	-Apply activation function after each layer.

	-Add output layer with 10 neurons (for 10 classes).

6.Define loss function: CrossEntropyLoss.

7.Define optimizer: Adam.

8.Training loop:

	-Forward pass: compute model output.

	-Compute loss between predicted and actual labels.

	-Backward pass: compute gradients.

	-Update weights using optimizer.

9.Testing phase:

	-Evaluate model on test data.

	-Calculate test loss and accuracy.

10.Plot results:

	-Training loss vs. epochs.

	-Test loss vs. epochs.

	-Accuracy vs. epochs.

Result:

The MLP model was successfully trained and tested on the MNIST/Fashion-MNIST dataset.

The model achieved around:

Accuracy: 95â€“98% (MNIST)

Accuracy: 85â€“90% (Fashion-MNIST)

Loss decreased steadily with epochs, and accuracy improved over time.

The plotted graphs clearly show the training and validation performance.

Conclusion:

The Multilayer Perceptron (MLP) is capable of learning complex, non-linear patterns in image data.
It successfully classifies handwritten digits and fashion items with high accuracy.
Using ReLU activation and Adam optimizer gives better convergence compared to Sigmoid or Tanh.

Viva / Oral Questions and Answers:

Q1. What is an MLP?
A Multilayer Perceptron is a feedforward neural network with one or more hidden layers between the input and output.

Q2. Why are activation functions used?
To introduce non-linearity, helping the network learn complex patterns.

Q3. Which activation function is most commonly used?
ReLU, because it avoids the vanishing gradient problem and is computationally efficient.

Q4. What loss function is used in MLP?
Cross Entropy Loss for multi-class classification.

Q5. What optimizer did you use?
Adam Optimizer (Adaptive Moment Estimation), as it converges faster than SGD.

Q6. Why do we normalize data?
To bring all pixel values to a common scale (âˆ’1 to 1) for stable and faster training.

Q7. What is the difference between MNIST and Fashion-MNIST?
MNIST has handwritten digits (0â€“9), while Fashion-MNIST contains grayscale clothing images.

Q8. What is the role of the learning rate?
It controls how much the weights are updated during training.

Q9. What happens if we increase the number of hidden layers?
The model may learn better representations but can also overfit or train slower.

Q10. Can an MLP handle image spatial structure well?
Not very efficiently â€” CNNs (Convolutional Neural Networks) handle spatial patterns better.