Practical No. 4 – Comparison of Activation Functions and Gradient Flow
Aim:

To implement a Multilayer Perceptron (MLP) using different activation functions (Sigmoid, Tanh, and ReLU) on the MNIST dataset,
and to compare their effects on training loss, accuracy, and gradient norms.

************************************************************************************

✅ Theory — Code (what each part does)
1. Device & libraries

Selects GPU if available (device = cuda) otherwise CPU. This speeds up training when a GPU exists.

2. Data loading & transforms

Loads MNIST train & test sets.

ToTensor() converts images to tensors; Normalize((0.5,), (0.5,)) scales pixels to ~[-1,1].

train_loader yields shuffled mini-batches of 64 for training; test_loader exists but is not used for evaluation in this script.

3. MLP model

MLP is a simple fully-connected network:

Input: 28×28 flattened → 784 features

Hidden1: 256 neurons → activation

Hidden2: 128 neurons → activation

Output: 10 logits (one per digit class)

The activation function is passed in (Sigmoid / Tanh / ReLU), making the model reusable for experiments.

4. train_model function

Creates model → moves it to device.

Uses Adam optimizer and CrossEntropyLoss.

For each epoch and batch:

Forward pass → compute logits.

Compute loss → loss.backward() to accumulate gradients.

Compute gradient norm: loops over parameters, gets each p.grad.data.norm(2), squares and sums, then takes square root → Euclidean norm of the gradient vector. This is appended per batch.

optimizer.step() updates weights.

Track training loss (per batch) and accuracy (per epoch).

Returns: lists of per-iteration losses, per-epoch accuracies, per-iteration gradient norms.

5. Experiments loop

Runs train_model separately for three activations: Sigmoid, Tanh, ReLU so you can compare training dynamics.

6. Visualization

Three subplots:

Training Loss vs iteration (plots batch losses collected across epochs).

Training Accuracy vs epoch (one value per epoch, per activation).

Gradient Norms vs iteration (how the magnitude of gradients changes over training).

✅ Theory — Output (what the results mean & what to expect)
A. Training Loss plot

Shows how the batch loss decreases during training.

ReLU typically shows faster, more stable decrease.

Sigmoid and Tanh may decrease slower (and can plateau) because of saturation (outputs near ±1 lead to tiny gradients).

B. Training Accuracy plot

Accuracy is computed on the training set at the end of each epoch.

Expect:

ReLU → fastest rise in accuracy and highest final training accuracy.

Tanh → good performance, often better than Sigmoid because it’s zero-centered.

Sigmoid → slower improvement, may underperform due to vanishing gradients.

Note: These are training accuracies. No test/validation accuracy is shown; you should evaluate on test_loader to measure generalization.

C. Gradient Norms plot

Shows the Euclidean norm of the gradient vector per batch.

Interpretation:

If norms → very small (near 0): vanishing gradients (training slows/stalls). Sigmoid/Tanh can cause this when units saturate.

If norms → very large or explode: risk of unstable updates; may need gradient clipping.

ReLU tends to keep usable gradient magnitudes (not vanishing for positive activations), so gradient norms often remain healthy.

Tracking grad norms helps diagnose optimization issues (vanishing/exploding gradients, bad learning rates).

**************************************************************************

Theory:

An Activation Function introduces non-linearity into a neural network, enabling it to learn complex, non-linear patterns in the data.
Without activation functions, neural networks would behave like simple linear models and could not solve complex tasks.

1️⃣ Purpose of Activation Functions:

Introduce non-linearity in the model.

Help the network learn complex mappings between input and output.

Control the flow of gradients during backpropagation.

2️⃣ Types of Activation Functions Used:

A) Sigmoid Function

​
Output range: (0, 1)

Advantage: Smooth and used for binary classification.

Disadvantage: Causes vanishing gradients for large input values.

Derivative: 


B) Tanh Function

Output range: (-1, 1)

Advantage: Zero-centered (helps in faster convergence).

Disadvantage: Still suffers from vanishing gradients for large |x| values.

C) ReLU (Rectified Linear Unit)



Output range: [0, ∞)

Advantage: Fast convergence, avoids vanishing gradient problem.

Disadvantage: Can cause dying ReLU problem (neurons stuck at 0).

3️⃣ Gradient Flow:

During backpropagation, gradients are multiplied through many layers.

If gradients are too small → Vanishing Gradient Problem (weights stop updating).

If gradients are too large → Exploding Gradient Problem (unstable training).

Monitoring the gradient norm (magnitude of gradients) helps detect this issue.

4️⃣ Expected Behavior:
Activation		Speed of Convergence		Gradient Flow			Accuracy			Comment
Sigmoid		Slow						Very small (vanishing)	Low			Not preferred for deep networks
Tanh			Moderate					Better than sigmoid		Moderate		Zero-centered but still vanishes
ReLU		Fast						Stable gradients		High			Best for most cases
Algorithm:

1.Import required libraries – torch, torchvision, matplotlib, etc.

2.Load Dataset – MNIST (handwritten digits 0–9).

-Apply transformations: ToTensor() and normalization.

-Create DataLoaders for training and testing.

3.Define MLP Model:

-3 layers: 784 → 256 → 128 → 10.

-Use one of the activation functions (Sigmoid, Tanh, ReLU).

4.Define Loss Function – CrossEntropyLoss.

5.Define Optimizer – Adam optimizer with learning rate = 0.001.

6.Train the Model:
a. Perform forward pass (compute output).
b. Compute loss using criterion.
c. Backpropagate using loss.backward().
d. Calculate gradient norm to monitor gradient behavior.
e. Update weights using optimizer.step().
f. Record loss and accuracy for each epoch.

7.Repeat for each activation function (Sigmoid, Tanh, ReLU).

8.Plot Results:

-Loss vs Iterations

-Accuracy vs Epochs

-Gradient Norm vs Iterations

9.Compare and analyze training performance for each activation.


Observation (Expected Results):

ReLU converges faster with higher accuracy (~97–98%).

Tanh performs moderately well (~94–95%).

Sigmoid converges slowly with lower accuracy (~90–92%).

Gradient norms for Sigmoid and Tanh are smaller → Vanishing gradient observed.

Gradient norms for ReLU are stable and higher → Healthy gradient flow.

Conclusion:

The choice of activation function significantly affects the learning speed, gradient behavior, and accuracy of a neural network.

ReLU provides faster convergence and better gradient flow, making it ideal for most deep learning models.

Sigmoid and Tanh functions are prone to vanishing gradients, resulting in slower learning and lower accuracy.

Monitoring gradient norms helps in detecting training instabilities (vanishing or exploding gradients).

Viva / Oral Questions and Answers:

Q1. What is the role of an activation function in a neural network?
A: It introduces non-linearity into the model and allows the network to learn complex relationships.

Q2. Why do we need non-linear activation functions?
A: Without them, the network becomes a simple linear model regardless of depth.

Q3. What is the difference between Sigmoid and ReLU?
A: Sigmoid saturates for large values (causing vanishing gradients), while ReLU is faster and avoids this issue.

Q4. What is the vanishing gradient problem?
A: It occurs when gradients become too small during backpropagation, preventing effective learning in early layers.

Q5. What is the exploding gradient problem?
A: When gradients grow too large, causing unstable updates and potential NaN values.

Q6. How does ReLU help avoid vanishing gradients?
A: For positive inputs, its derivative is 1, maintaining consistent gradient flow.

Q7. What is the disadvantage of ReLU?
A: Some neurons may permanently output zero (dying ReLU problem).

Q8. Which activation is zero-centered?
A: Tanh is zero-centered, whereas Sigmoid outputs between 0 and 1.

Q9. What is the output range of each activation?
A: Sigmoid (0,1), Tanh (-1,1), ReLU [0,∞).

Q10. Which activation function performed best in this experiment?

A: ReLU — because it provided stable gradients, faster learning, and higher accuracy.
