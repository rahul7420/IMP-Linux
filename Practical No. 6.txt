Practical No. 6 â€“ Implementation of Recurrent Neural Network (RNN/LSTM)
Aim:

To implement a Recurrent Neural Network (RNN) or Long Short-Term Memory (LSTM) model using PyTorch for sequence or text data classification, and to understand how it handles sequential dependencies.

Theory:
1ï¸âƒ£ Introduction:

Traditional neural networks (like MLPs and CNNs) assume that all inputs are independent.
However, many real-world problems have sequential or temporal relationships, such as:

Text (words in a sentence)

Speech (sound over time)

Time series (stock prices, sensor readings)

A Recurrent Neural Network (RNN) is designed to handle such sequential data by maintaining an internal memory (hidden state) that captures information from previous steps.

2ï¸âƒ£ RNN Architecture:

An RNN processes input sequences one element at a time.
At each time step t, it takes input xâ‚œ, the previous hidden state hâ‚œâ‚‹â‚, and produces a new hidden state hâ‚œ as:

â„
ğ‘¡
=
ğ‘“
(
ğ‘Š
ğ‘¥
â„
ğ‘¥
ğ‘¡
+
ğ‘Š
â„
â„
â„
ğ‘¡
âˆ’
1
+
ğ‘
â„
)
h
t
	â€‹

=f(W
xh
	â€‹

x
t
	â€‹

+W
hh
	â€‹

h
tâˆ’1
	â€‹

+b
h
	â€‹

)
ğ‘¦
ğ‘¡
=
ğ‘Š
â„
ğ‘¦
â„
ğ‘¡
+
ğ‘
ğ‘¦
y
t
	â€‹

=W
hy
	â€‹

h
t
	â€‹

+b
y
	â€‹


Here:

xâ‚œ: input at time step t

hâ‚œ: hidden state at time step t

W: weight matrices

f: activation function (usually tanh or ReLU)

3ï¸âƒ£ Limitations of Vanilla RNN:

Vanishing Gradient Problem: Gradients become very small while training on long sequences, making it difficult to learn long-term dependencies.

Exploding Gradient Problem: Gradients grow excessively large, causing unstable updates.

To overcome these issues, LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) were developed.

4ï¸âƒ£ LSTM (Long Short-Term Memory):

An LSTM is an advanced RNN architecture that can remember information for long periods using gates:

Forget Gate: Decides what information to discard.

Input Gate: Decides what new information to store.

Output Gate: Decides what to output at the current step.

LSTM equations:

ğ‘“
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘“
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘“
)
f
t
	â€‹

=Ïƒ(W
f
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
f
	â€‹

)
ğ‘–
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘–
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘–
)
i
t
	â€‹

=Ïƒ(W
i
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
i
	â€‹

)
ğ¶
ğ‘¡
~
=
tanh
â¡
(
ğ‘Š
ğ¶
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ¶
)
C
t
	â€‹

~
	â€‹

=tanh(W
C
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
C
	â€‹

)
ğ¶
ğ‘¡
=
ğ‘“
ğ‘¡
âˆ—
ğ¶
ğ‘¡
âˆ’
1
+
ğ‘–
ğ‘¡
âˆ—
ğ¶
ğ‘¡
~
C
t
	â€‹

=f
t
	â€‹

âˆ—C
tâˆ’1
	â€‹

+i
t
	â€‹

âˆ—
C
t
	â€‹

~
	â€‹

ğ‘œ
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘œ
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘œ
)
o
t
	â€‹

=Ïƒ(W
o
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
o
	â€‹

)
â„
ğ‘¡
=
ğ‘œ
ğ‘¡
âˆ—
tanh
â¡
(
ğ¶
ğ‘¡
)
h
t
	â€‹

=o
t
	â€‹

âˆ—tanh(C
t
	â€‹

)

Where:

Ïƒ = sigmoid activation

* = element-wise multiplication

Câ‚œ = cell state (long-term memory)

5ï¸âƒ£ Applications of RNN/LSTM:

Text generation

Sentiment analysis

Machine translation

Speech recognition

Time series forecasting

Algorithm:

1.Import Libraries:

-torch, torch.nn, torch.optim, torchtext (or custom text data).

2.Data Preparation:

-Preprocess input sequences (e.g., sentences, characters).

-Tokenize and convert to numerical form (word indices).

-Create DataLoader for training and testing.

3.Model Definition:

-Define an RNN or LSTM class.

-Input layer â†’ Embedding layer â†’ RNN/LSTM â†’ Fully connected output layer.
Example (LSTM):

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        out, (h, c) = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out


4.Loss and Optimizer:

-Loss: CrossEntropyLoss

-Optimizer: Adam or SGD

5.Training Loop:

-For each epoch:

	-Forward pass through the RNN/LSTM.

	-Compute loss.

	-Backward pass and update weights.

	-Calculate accuracy.

6.Testing Phase:

-Evaluate model on test data.

-Measure loss and accuracy.

7.Plot Results:

-Plot training and testing loss.

-Plot training and testing accuracy over epochs.


Result:

The RNN/LSTM model successfully learns temporal patterns from sequential data.

It achieves high accuracy and stable convergence compared to simple feedforward networks.

Example Output:

Epoch [5/5], Train Loss: 0.1224, Train Acc: 95.2%, Test Acc: 94.7%

Conclusion:

RNNs are useful for processing sequential data but suffer from vanishing/exploding gradients.

LSTM networks solve this by introducing gates that control memory and forget mechanisms.

The experiment demonstrates how LSTM effectively handles long-term dependencies, achieving better performance than vanilla RNNs.

Viva / Oral Questions and Answers:

Q1. What is an RNN?
A: A Recurrent Neural Network processes sequential data by maintaining a hidden state that captures previous context.

Q2. Why are RNNs used for text data?
A: Because they remember previous inputs, helping understand context in sequences (e.g., sentences).

Q3. What is the difference between RNN and LSTM?
A: LSTM uses gates (input, forget, output) to store long-term information, while RNNs can forget old information due to vanishing gradients.

Q4. What is the vanishing gradient problem?
A: Gradients become very small during backpropagation through time, preventing effective learning of long sequences.

Q5. How does LSTM solve vanishing gradients?
A: LSTM introduces a cell state and gating mechanism that preserve gradients over long time steps.

Q6. What is the role of the forget gate in LSTM?
A: It decides which past information to remove from the cell state.

Q7. What is the purpose of the embedding layer?
A: It converts words or tokens into dense numerical vectors suitable for neural network input.

Q8. Which loss function is used?
A: Cross Entropy Loss for classification tasks.

Q9. What optimizer is commonly used?
A: Adam Optimizer, as it provides faster convergence and adaptive learning rates.

Q10. What are the main applications of LSTM?
A: Sentiment analysis, machine translation, time series prediction, speech recognition, and text generation.