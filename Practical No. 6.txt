Practical No. 6 ‚Äì Implementation of Recurrent Neural Network (RNN/LSTM)
Aim:

To implement a Recurrent Neural Network (RNN) or Long Short-Term Memory (LSTM) model using PyTorch for sequence or text data classification, and to understand how it handles sequential dependencies.

*****************************************************************************************************

üü¶ A) Code Theory ‚Äî what each block does (concise)

Imports & setup

tensorflow.keras + utilities for datasets, preprocessing, models, and layers.

max_features = 10000 keeps only the top 10k words in the IMDB vocabulary.

maxlen = 200 pads/truncates every review to length 200 tokens.

Load & pad data

imdb.load_data(num_words=max_features) returns sequences of word indices (already integer-encoded).

sequence.pad_sequences(..., maxlen=200) makes fixed-length inputs required by the model.

Model architecture

Embedding(max_features, 128) maps integers ‚Üí 128-dim trainable vectors.

SimpleRNN(128, dropout=0.2) is a vanilla recurrent layer with 128 hidden units. (Note: dropout here is input dropout; recurrent_dropout is for recurrent state).

Dense(1, activation='sigmoid') outputs probability for positive sentiment (binary).

Compile & training

loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'].

model.fit(...) trains for multiple epochs. You experimented with both batch_size 32 and 64.

Interactive input

imdb.get_word_index() returns a mapping word ‚Üí index for tokenizing custom text.

encode_review() maps raw words to indices with an offset and pads to maxlen.

model.predict(processed_review) returns a probability; >0.5 interpreted as Positive.

Re-training & final evaluation

You re-trained with batch_size 64 for 5 epochs and printed final test accuracy.

üü© B) Output Theory ‚Äî what the numbers mean

Training accuracy reflects model performance on seen training examples.

Validation/test accuracy measures generalization to unseen data.

Your runs show training accuracy increasing (up to ~91% in last run) while test accuracy hovers around ~76‚Äì80% ‚Äî this suggests some generalization gap (possible overfitting, or model capacity / data mismatch).

Epoch logs (loss & accuracy) help diagnose:

If train ‚Üë, test ‚Üë ‚Äî model is learning and generalizing.

If train ‚Üë, test ‚Üì or flat ‚Äî model may be overfitting.

The interactive prediction (confidence ‚âà 0.797) shows the model assigns a decent probability to the input being positive.

****************************************************************************************************************

Theory:
1Ô∏è‚É£ Introduction:

Traditional neural networks (like MLPs and CNNs) assume that all inputs are independent.
However, many real-world problems have sequential or temporal relationships, such as:

Text (words in a sentence)

Speech (sound over time)

Time series (stock prices, sensor readings)

A Recurrent Neural Network (RNN) is designed to handle such sequential data by maintaining an internal memory (hidden state) that captures information from previous steps.

2Ô∏è‚É£ RNN Architecture:

An RNN processes input sequences one element at a time.

x‚Çú: input at time step t

h‚Çú: hidden state at time step t

W: weight matrices

f: activation function (usually tanh or ReLU)

3Ô∏è‚É£ Limitations of Vanilla RNN:

Vanishing Gradient Problem: Gradients become very small while training on long sequences, making it difficult to learn long-term dependencies.

Exploding Gradient Problem: Gradients grow excessively large, causing unstable updates.

To overcome these issues, LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) were developed.

4Ô∏è‚É£ LSTM (Long Short-Term Memory):

An LSTM is an advanced RNN architecture that can remember information for long periods using gates:

Forget Gate: Decides what information to discard.

Input Gate: Decides what new information to store.

Output Gate: Decides what to output at the current step.


5Ô∏è‚É£ Applications of RNN/LSTM:

Text generation

Sentiment analysis

Machine translation

Speech recognition

Time series forecasting

Algorithm:

1.Import Libraries:

-torch, torch.nn, torch.optim, torchtext (or custom text data).

2.Data Preparation:

-Preprocess input sequences (e.g., sentences, characters).

-Tokenize and convert to numerical form (word indices).

-Create DataLoader for training and testing.

3.Model Definition:

-Define an RNN or LSTM class.

-Input layer ‚Üí Embedding layer ‚Üí RNN/LSTM ‚Üí Fully connected output layer.
Example (LSTM):

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        out, (h, c) = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out


4.Loss and Optimizer:

-Loss: CrossEntropyLoss

-Optimizer: Adam or SGD

5.Training Loop:

-For each epoch:

	-Forward pass through the RNN/LSTM.

	-Compute loss.

	-Backward pass and update weights.

	-Calculate accuracy.

6.Testing Phase:

-Evaluate model on test data.

-Measure loss and accuracy.

7.Plot Results:

-Plot training and testing loss.

-Plot training and testing accuracy over epochs.


Result:

The RNN/LSTM model successfully learns temporal patterns from sequential data.

It achieves high accuracy and stable convergence compared to simple feedforward networks.

Example Output:

Epoch [5/5], Train Loss: 0.1224, Train Acc: 95.2%, Test Acc: 94.7%

Conclusion:

RNNs are useful for processing sequential data but suffer from vanishing/exploding gradients.

LSTM networks solve this by introducing gates that control memory and forget mechanisms.

The experiment demonstrates how LSTM effectively handles long-term dependencies, achieving better performance than vanilla RNNs.

Viva / Oral Questions and Answers:

Q1. What is an RNN?
A: A Recurrent Neural Network processes sequential data by maintaining a hidden state that captures previous context.

Q2. Why are RNNs used for text data?
A: Because they remember previous inputs, helping understand context in sequences (e.g., sentences).

Q3. What is the difference between RNN and LSTM?
A: LSTM uses gates (input, forget, output) to store long-term information, while RNNs can forget old information due to vanishing gradients.

Q4. What is the vanishing gradient problem?
A: Gradients become very small during backpropagation through time, preventing effective learning of long sequences.

Q5. How does LSTM solve vanishing gradients?
A: LSTM introduces a cell state and gating mechanism that preserve gradients over long time steps.

Q6. What is the role of the forget gate in LSTM?
A: It decides which past information to remove from the cell state.

Q7. What is the purpose of the embedding layer?
A: It converts words or tokens into dense numerical vectors suitable for neural network input.

Q8. Which loss function is used?
A: Cross Entropy Loss for classification tasks.

Q9. What optimizer is commonly used?
A: Adam Optimizer, as it provides faster convergence and adaptive learning rates.

Q10. What are the main applications of LSTM?

A: Sentiment analysis, machine translation, time series prediction, speech recognition, and text generation.
