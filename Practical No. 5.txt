Practical No. 5 – Implementation of Convolutional Neural Network (CNN)
Aim:

To implement a Convolutional Neural Network (CNN) using PyTorch for image classification on the MNIST or CIFAR-10 dataset, and compare its performance with a simple Multilayer Perceptron (MLP).

Theory:
1️⃣ Introduction:

A Convolutional Neural Network (CNN) is a deep learning model specifically designed for analyzing visual data such as images.
Unlike traditional MLPs, CNNs automatically learn spatial hierarchies of features — edges, textures, shapes — by using convolutional filters.

CNNs are highly effective for tasks like image recognition, object detection, and image classification.

2️⃣ Why CNN instead of MLP?

MLP treats all pixels independently and ignores spatial relationships.

CNN uses convolutional filters to extract local features like edges and corners.

CNNs have fewer parameters due to weight sharing and local connectivity.

CNNs generalize better and are computationally efficient for image data.

3️⃣ Architecture of a CNN:

A typical CNN includes the following layers:

Input Layer:

Accepts image data (e.g., 28×28 grayscale image).

Convolutional Layer (Conv Layer):

Performs convolution between the input and learnable filters (kernels).

********************************

Detailed explanation of key functions used (one-line each)

1.mnist.load_data() — downloads and returns train/test splits.

2.reshape(...,1) — adds channel dimension for Conv2D.

3./ 255 — scale to [0,1].

4.to_categorical() — convert integer labels to one-hot vectors.

5.ImageDataGenerator(...) — specify augmentation transforms.

6.datagen.flow(x_train, y_train, batch_size=64) — yields augmented batches for training.

7.Conv2D(filters, kernel, activation) — convolution layer producing feature maps.

8.MaxPooling2D(pool) — downsample spatial dimensions.

9.Flatten() — convert 3D feature maps to 1D vector.

10.Dense(units, activation) — fully connected layer.

11.softmax — converts logits to probabilities that sum to 1.

12.compile(optimizer, loss) — configure training: optimizer and loss.

13.fit(...) — train the model.

14.evaluate(...) — compute metrics on test set.

**********************************



Detects features such as edges and corners.

Activation Function (ReLU):

Adds non-linearity to the network.

ReLU(x) = max(0, x).

Pooling Layer (Max Pooling):

Reduces spatial size and computation.

Example: 2×2 max pooling reduces feature map size by half.

Fully Connected (FC) Layer:

Flattens features and maps them to the output classes.

Output Layer:

Uses Softmax for multi-class classification.

4️⃣ Advantages of CNNs:

Automatically learn relevant image features.

Reduce number of parameters through weight sharing.

Provide translation and scale invariance.

Achieve higher accuracy than fully connected networks for visual data.

**Algorithm:

1.Import Libraries:
-Import torch, torchvision, torch.nn, torch.optim, and matplotlib.

2.Load Dataset:

-Use MNIST or CIFAR-10 dataset.

-Normalize pixel values and create DataLoader objects for training and testing.

3.Define CNN Architecture:

-Add two convolutional layers (Conv2D) with ReLU and MaxPooling.

-Add fully connected layers to classify into output classes.

-Example:

Conv2d(1, 32, kernel_size=3)
ReLU
MaxPool2d(2)
Conv2d(32, 64, kernel_size=3)
ReLU
MaxPool2d(2)
Flatten
Linear → ReLU → Linear → Output


4.Define Loss and Optimizer:

-Loss: CrossEntropyLoss

-Optimizer: Adam or SGD

5.Training Phase:

-For each epoch:

	-Forward pass: Compute output and loss.

	-Backward pass: Compute gradients.

	-Update weights with optimizer.

	-Record training loss and accuracy.

6.Testing Phase:

-Evaluate the model on the test dataset.

-Compute test loss and accuracy.

7.Plot Results:

-Plot training & test loss vs. epochs.

-Plot training & test accuracy vs. epochs.

8.Compare Results:

Compare CNN’s performance with a simple MLP.


Result:

CNN successfully classifies handwritten digits or images with high accuracy.

Loss decreases and accuracy increases with each epoch.

Example result:

Epoch 5/5: Train Loss = 0.042, Train Acc = 99.15%, Test Acc = 98.75%


CNN shows higher accuracy and faster convergence than MLP due to feature extraction via convolutional layers.

Conclusion:

CNN automatically learns spatial and local features of images, unlike MLP which relies on flattened input.

The use of convolution and pooling layers reduces parameters and improves efficiency.

The experiment proves that CNNs achieve better accuracy and generalization for image classification tasks.

CNNs form the foundation for advanced models like ResNet, VGG, and Inception networks.

What to say in viva — short, memorized answers

Q: Why normalize images?
A: To scale inputs in a similar range so training is numerically stable and converges faster.

Q: Why use softmax + categorical_crossentropy?
A: Softmax produces class probabilities; categorical cross-entropy measures the distance between predicted distribution and true one-hot label.

Q: Why does CNN perform better than MLP on images?
A: CNN uses local receptive fields and weight sharing (filters) to learn spatial features efficiently; MLP ignores spatial structure.

Q: What does ImageDataGenerator do?**
A: It applies random transformations to training images each epoch (augmentation) to improve generalization.

Q: Why use Adam optimizer?**
A: Adam adapts learning rates per parameter and includes momentum — often converges faster and requires little tuning.

Q: What is overfitting and how to avoid it?**
A: Overfitting is when model fits training data too well but fails on new data. Avoid with augmentation, dropout, L2, early stopping.

Q: What is a filter in Conv2D?**
A: A small matrix (kernel) convolved with the image to detect patterns like edges and textures.

Q: What is pooling for?**
A: Reduces spatial dimensions and makes features robust to small translations.
